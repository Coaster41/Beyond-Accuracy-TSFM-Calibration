{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce558dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gift_eval.data import Dataset, Term\n",
    "\n",
    "# -----------------------\n",
    "# Configuration\n",
    "# -----------------------\n",
    "min_len = 512 + 256  # length of each sampled window\n",
    "stride = 128         # ONLY used to compute how many samples to draw\n",
    "max_per_dataset = 1024\n",
    "seed = 42            # for reproducibility\n",
    "\n",
    "# Synthetic time index per sampled window\n",
    "base_start = pd.Timestamp(\"2000-01-01\")\n",
    "synthetic_freq = \"H\"  # hourly\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# -----------------------\n",
    "# Helpers\n",
    "# -----------------------\n",
    "def find_subdatasets(root: Path) -> List[str]:\n",
    "    names = []\n",
    "    for info in root.rglob(\"dataset_info.json\"):\n",
    "        state = info.parent / \"state.json\"\n",
    "        if state.exists():\n",
    "            names.append(str(info.parent.relative_to(root)))\n",
    "    return sorted(set(names))\n",
    "\n",
    "def iter_univariate_series(ds_obj: Dataset):\n",
    "    \"\"\"\n",
    "    Yield tuples (row_idx, length) for every series in the dataset,\n",
    "    treating multivariate items as univariate by taking ONLY the last feature.\n",
    "\n",
    "    For univariate items, it's just the single series.\n",
    "    For multivariate items (D, T), we use the last dimension: target[-1].\n",
    "    \"\"\"\n",
    "    num_rows = ds_obj.hf_dataset.num_rows\n",
    "    for i in range(num_rows):\n",
    "        target = ds_obj.hf_dataset[i][\"target\"]  # numpy array due to with_format(\"numpy\")\n",
    "        # If multivariate: (D, T), use the last feature\n",
    "        if getattr(target, \"ndim\", 1) > 1:\n",
    "            y = np.asarray(target[-1])\n",
    "        else:\n",
    "            y = np.asarray(target)\n",
    "        yield (i, y.shape[-1])\n",
    "\n",
    "def get_series_array(ds_obj: Dataset, row_idx: int):\n",
    "    \"\"\"\n",
    "    Return the 1D numpy array for the series at row_idx,\n",
    "    using ONLY the last feature if the target is multivariate.\n",
    "    \"\"\"\n",
    "    target = ds_obj.hf_dataset[row_idx][\"target\"]\n",
    "    if getattr(target, \"ndim\", 1) > 1:\n",
    "        return np.asarray(target[-1])\n",
    "    else:\n",
    "        return np.asarray(target)\n",
    "\n",
    "def candidate_count_for_length(T: int, min_len: int, stride: int) -> int:\n",
    "    \"\"\"\n",
    "    Number of windows to sample for this series.\n",
    "    We DO NOT align starts to stride; stride is only used to compute how many samples.\n",
    "    \"\"\"\n",
    "    if T < min_len:\n",
    "        return 0\n",
    "    return 1 + (T - min_len) // stride\n",
    "\n",
    "def proportional_allocation(total_picks: int, counts: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Allocate 'total_picks' integers across series proportional to 'counts',\n",
    "    ensuring sum(...) == total_picks and n_i <= counts[i].\n",
    "    Uses flooring + distribute remainder to largest fractional parts.\n",
    "    \"\"\"\n",
    "    if total_picks <= 0 or counts.sum() == 0:\n",
    "        return np.zeros_like(counts, dtype=int)\n",
    "\n",
    "    ideal = total_picks * (counts / counts.sum())\n",
    "    floor_alloc = np.floor(ideal).astype(int)\n",
    "    remainder = total_picks - floor_alloc.sum()\n",
    "\n",
    "    if remainder > 0:\n",
    "        fract = ideal - floor_alloc\n",
    "        order = np.argsort(-fract)  # descending by fractional part\n",
    "        for idx in order:\n",
    "            if remainder == 0:\n",
    "                break\n",
    "            if floor_alloc[idx] < counts[idx]:\n",
    "                floor_alloc[idx] += 1\n",
    "                remainder -= 1\n",
    "\n",
    "    floor_alloc = np.minimum(floor_alloc, counts)\n",
    "    return floor_alloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b21475",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = # ROOT PATH\n",
    "names = find_subdatasets(root)\n",
    "print(f\"Found {len(names)} subdatasets\")\n",
    "\n",
    "all_windows = []      # list of 1D numpy arrays (length = min_len)\n",
    "all_datasets = []     # dataset name for each window (same length as all_windows)\n",
    "\n",
    "for n in names:\n",
    "    try:\n",
    "        ds_obj = Dataset(name=n, term=Term.SHORT, to_univariate=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {n}: failed to load -> {e}\")\n",
    "        continue\n",
    "\n",
    "    # Collect metadata for series (multivariate collapsed to last feature)\n",
    "    meta = []  # (row_idx, length, num_candidates)\n",
    "    for row_idx, T in iter_univariate_series(ds_obj):\n",
    "        num_candidates = candidate_count_for_length(T, min_len, stride)\n",
    "        if num_candidates > 0:\n",
    "            meta.append((row_idx, T, num_candidates))\n",
    "\n",
    "    if not meta:\n",
    "        print(f\"{n}: no series with length >= {min_len}, skipping\")\n",
    "        continue\n",
    "\n",
    "    # Total candidates across this dataset\n",
    "    candidate_counts = np.array([m[2] for m in meta], dtype=int)\n",
    "    tot_candidates = int(candidate_counts.sum())\n",
    "    k = min(max_per_dataset, tot_candidates)\n",
    "    if k <= 0:\n",
    "        print(f\"{n}: no candidate windows, skipping\")\n",
    "        continue\n",
    "\n",
    "    # Allocate how many windows to sample from each series proportionally\n",
    "    alloc = proportional_allocation(k, candidate_counts)\n",
    "    if alloc.sum() == 0:\n",
    "        print(f\"{n}: allocation produced zero samples, skipping\")\n",
    "        continue\n",
    "\n",
    "    # For each series, sample starts NON-deterministically across all eligible positions\n",
    "    sampled_this_dataset = 0\n",
    "    for (row_idx, T, n_cand), n_i in zip(meta, alloc):\n",
    "        if n_i <= 0:\n",
    "            continue\n",
    "\n",
    "        # Eligible starting positions are EVERY integer position where a window fits:\n",
    "        # [0, 1, 2, ..., T - min_len]\n",
    "        eligible_size = T - min_len + 1\n",
    "        if eligible_size <= 0:\n",
    "            continue\n",
    "\n",
    "        # stride ONLY determines how many samples to draw\n",
    "        n_i = min(n_i, eligible_size)\n",
    "\n",
    "        # Sample without replacement from the full eligible range\n",
    "        # Use Python's random for reproducibility with the given seed\n",
    "        chosen_starts = random.sample(range(eligible_size), k=n_i)\n",
    "\n",
    "        # Retrieve the full series and slice windows\n",
    "        y_full = get_series_array(ds_obj, row_idx)\n",
    "        for s in chosen_starts:\n",
    "            window = y_full[s : s + min_len]\n",
    "            if len(window) == min_len:\n",
    "                all_windows.append(window.astype(float, copy=False))\n",
    "                all_datasets.append(n)\n",
    "                sampled_this_dataset += 1\n",
    "\n",
    "    print(f\"{n}: sampled {sampled_this_dataset} windows (k={k}, tot_candidates={tot_candidates})\")\n",
    "\n",
    "print(f\"Total sampled windows across all datasets: {len(all_windows)}\")\n",
    "\n",
    "# -----------------------\n",
    "# Build long DataFrame\n",
    "# -----------------------\n",
    "if len(all_windows) == 0:\n",
    "    print(\"No windows sampled. Check min_len/stride/max_per_dataset settings.\")\n",
    "else:\n",
    "    num_windows = len(all_windows)\n",
    "    Y = np.vstack(all_windows)  # shape: (num_windows, min_len)\n",
    "\n",
    "    base_index = pd.date_range(base_start, periods=min_len, freq=synthetic_freq)\n",
    "\n",
    "    unique_ids = np.arange(num_windows, dtype=int)\n",
    "    unique_id_col = np.repeat(unique_ids, min_len)\n",
    "    dataset_col = np.repeat(np.array(all_datasets, dtype=object), min_len)\n",
    "    ds_col = np.tile(base_index.values, num_windows)\n",
    "    y_col = Y.reshape(-1)\n",
    "\n",
    "    long_df = pd.DataFrame(\n",
    "        {\n",
    "            \"ds\": ds_col,\n",
    "            \"y\": y_col,\n",
    "            \"unique_id\": unique_id_col,\n",
    "            \"dataset\": dataset_col,   # keep source dataset name\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(long_df.head())\n",
    "    print(f\"Long DF shape: {long_df.shape} (expected ~ {min_len} * total_windows)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ebdf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv\n",
    "out_dir = # PATH\n",
    "long_df.to_csv(f\"{out_dir}/y_gift-eval-subsample.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e5c253",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
